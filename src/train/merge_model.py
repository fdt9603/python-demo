"""
Day 3: æ¨¡å‹åˆå¹¶è„šæœ¬
åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹ï¼Œå¹¶å›ºåŒ–JSONæ ¼å¼çº¦æŸ
"""
import torch
import json
import os
import glob
from peft import PeftModel, LoraConfig, get_peft_model
from peft.utils import set_peft_model_state_dict
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from PIL import Image

# å°è¯•å¯¼å…¥ safetensors
try:
    from safetensors.torch import load_file as safe_load_file
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False
    print("âš ï¸  safetensorsæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ ‡å‡†torch.load")

# å°è¯•å¯¼å…¥ ModelScope
try:
    from modelscope import snapshot_download
    MODELSCOPE_AVAILABLE = True
except ImportError:
    MODELSCOPE_AVAILABLE = False
    print("âš ï¸  ModelScopeæœªå®‰è£…ï¼Œå°†ä½¿ç”¨HuggingFaceã€‚å¦‚éœ€ä½¿ç”¨ModelScopeï¼Œè¯·è¿è¡Œ: pip install modelscope")


def merge_lora_weights(
    base_model_name: str,
    lora_checkpoint: str,
    output_dir: str,
    trust_remote_code: bool = True,
    use_modelscope: bool = False
):
    """
    åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
    
    Args:
        base_model_name: åŸºç¡€æ¨¡å‹è·¯å¾„
        lora_checkpoint: LoRAæ£€æŸ¥ç‚¹è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        trust_remote_code: æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
        use_modelscope: æ˜¯å¦ä½¿ç”¨ModelScopeåŠ è½½ï¼ˆè§£å†³ç½‘ç»œé—®é¢˜ï¼‰
    """
    actual_model_path = base_model_name
    
    # ModelScope æ¨¡å‹æ˜ å°„
    modelscope_model_map = {
        "Qwen/Qwen3-VL-32B-Instruct": "qwen/Qwen3-VL-32B-Instruct",
        "qwen/Qwen3-VL-32B-Instruct": "qwen/Qwen3-VL-32B-Instruct",
    }
    
    # å¦‚æœä½¿ç”¨ModelScopeï¼Œå…ˆä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
    if use_modelscope and MODELSCOPE_AVAILABLE:
        if base_model_name in modelscope_model_map:
            modelscope_name = modelscope_model_map[base_model_name]
        elif base_model_name.startswith("Qwen/") or base_model_name.startswith("qwen/"):
            modelscope_name = base_model_name.replace("Qwen/", "qwen/")
        else:
            modelscope_name = base_model_name
        
        print(f"ğŸ”„ ä½¿ç”¨ModelScopeä¸‹è½½æ¨¡å‹: {modelscope_name}")
        try:
            local_model_path = snapshot_download(
                modelscope_name,
                cache_dir=os.getenv("MODELSCOPE_CACHE", "./modelscope_cache")
            )
            print(f"âœ… æ¨¡å‹å·²ä¸‹è½½åˆ°æœ¬åœ°: {local_model_path}")
            actual_model_path = local_model_path
        except Exception as e:
            print(f"âŒ ModelScopeä¸‹è½½å¤±è´¥: {e}")
            print("   å°†å°è¯•ä½¿ç”¨HuggingFaceåŠ è½½...")
            use_modelscope = False
    elif use_modelscope and not MODELSCOPE_AVAILABLE:
        print("âš ï¸  æœªå®‰è£… ModelScopeï¼Œè¯·è¿è¡Œ: pip install modelscope")
        print("   å°†å°è¯•ä½¿ç”¨ HuggingFace åŠ è½½ï¼ˆå¯èƒ½éœ€è¦ç½‘ç»œè¿æ¥ï¼‰")
    
    # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œç¦ç”¨ HuggingFace çš„åœ¨çº¿æ£€æŸ¥
    if os.path.exists(actual_model_path) or actual_model_path.startswith("./") or actual_model_path.startswith("/"):
        print(f"ğŸ“ æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œç¦ç”¨ HuggingFace åœ¨çº¿æ£€æŸ¥")
        os.environ["HF_HUB_OFFLINE"] = "1"
        os.environ["TRANSFORMERS_OFFLINE"] = "1"
    
    print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {actual_model_path}")
    
    model_kwargs = {
        "trust_remote_code": trust_remote_code,
        "torch_dtype": torch.float16,
        "device_map": "auto",
        "attn_implementation": "flash_attention_2",
    }
    
    # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
    if os.path.exists(actual_model_path) or actual_model_path.startswith("./") or actual_model_path.startswith("/"):
        model_kwargs["local_files_only"] = True
    
    try:
        model = Qwen3VLForConditionalGeneration.from_pretrained(
            actual_model_path,
            **model_kwargs
        )
    finally:
        # æ¢å¤ç¯å¢ƒå˜é‡
        if "HF_HUB_OFFLINE" in os.environ:
            del os.environ["HF_HUB_OFFLINE"]
        if "TRANSFORMERS_OFFLINE" in os.environ:
            del os.environ["TRANSFORMERS_OFFLINE"]
    
    print(f"åŠ è½½LoRAæƒé‡: {lora_checkpoint}")
    
    # æ£€æŸ¥ LoRA é…ç½®
    import json
    lora_config_path = os.path.join(lora_checkpoint, "adapter_config.json")
    if os.path.exists(lora_config_path):
        with open(lora_config_path, 'r', encoding='utf-8') as f:
            lora_config = json.load(f)
        print(f"   LoRAé…ç½®: r={lora_config.get('r')}, alpha={lora_config.get('lora_alpha')}")
    
    # å°è¯•åŠ è½½ LoRAï¼Œå¦‚æœé‡åˆ° AWQ å…¼å®¹æ€§é—®é¢˜ï¼Œä½¿ç”¨æ‰‹åŠ¨åŠ è½½
    try:
        model = PeftModel.from_pretrained(model, lora_checkpoint)
    except Exception as e:
        error_msg = str(e)
        if "awq" in error_msg.lower() or "PytorchGELUTanh" in error_msg or "cannot import name" in error_msg:
            print(f"âš ï¸  æ£€æµ‹åˆ° AWQ å…¼å®¹æ€§é—®é¢˜ï¼Œä½¿ç”¨æ‰‹åŠ¨åŠ è½½æ–¹å¼...")
            print(f"   é”™è¯¯: {error_msg[:150]}")
            
            # æ‰‹åŠ¨åŠ è½½ LoRA æƒé‡ï¼ˆç»•è¿‡ AWQ æ£€æŸ¥ï¼‰
            # æŸ¥æ‰¾æƒé‡æ–‡ä»¶
            lora_weight_files = (
                glob.glob(os.path.join(lora_checkpoint, "adapter_model*.safetensors")) +
                glob.glob(os.path.join(lora_checkpoint, "adapter_model*.bin"))
            )
            
            if not lora_weight_files:
                raise FileNotFoundError(f"æœªæ‰¾åˆ° LoRA æƒé‡æ–‡ä»¶: {lora_checkpoint}")
            
            print(f"   æ‰¾åˆ°æƒé‡æ–‡ä»¶: {os.path.basename(lora_weight_files[0])}")
            
            # è¯»å– LoRA é…ç½®
            lora_config_obj = LoraConfig.from_pretrained(lora_checkpoint)
            
            # åˆ›å»º PEFT æ¨¡å‹
            model = get_peft_model(model, lora_config_obj)
            
            # åŠ è½½æƒé‡
            weight_file = lora_weight_files[0]
            if weight_file.endswith('.safetensors') and SAFETENSORS_AVAILABLE:
                state_dict = safe_load_file(weight_file)
            else:
                state_dict = torch.load(weight_file, map_location="cpu")
            
            # è®¾ç½®æƒé‡
            set_peft_model_state_dict(model, state_dict)
            print(f"   âœ… æ‰‹åŠ¨åŠ è½½ LoRA æƒé‡æˆåŠŸ")
        else:
            # å…¶ä»–é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
            raise
    
    print("åˆå¹¶LoRAæƒé‡...")
    merged_model = model.merge_and_unload()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_dir}")
    merged_model.save_pretrained(output_dir, safe_serialization=True)
    
    # ä¿å­˜å¤„ç†å™¨ï¼ˆä½¿ç”¨å®é™…æ¨¡å‹è·¯å¾„ï¼‰
    processor_kwargs = {"trust_remote_code": trust_remote_code}
    if os.path.exists(actual_model_path) or actual_model_path.startswith("./") or actual_model_path.startswith("/"):
        processor_kwargs["local_files_only"] = True
    
    # ä¸´æ—¶ç¦ç”¨åœ¨çº¿æ£€æŸ¥
    if processor_kwargs.get("local_files_only", False):
        os.environ["HF_HUB_OFFLINE"] = "1"
        os.environ["TRANSFORMERS_OFFLINE"] = "1"
    
    try:
        processor = AutoProcessor.from_pretrained(actual_model_path, **processor_kwargs)
        processor.save_pretrained(output_dir)
    finally:
        if "HF_HUB_OFFLINE" in os.environ:
            del os.environ["HF_HUB_OFFLINE"]
        if "TRANSFORMERS_OFFLINE" in os.environ:
            del os.environ["TRANSFORMERS_OFFLINE"]
    
    # ä¿å­˜é…ç½®ï¼ˆæ·»åŠ JSONæ ¼å¼çº¦æŸï¼‰
    config_path = os.path.join(output_dir, "config.json")
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
    else:
        config = {}
    
    # æ·»åŠ JSONæ ¼å¼çº¦æŸé…ç½®
    config["forced_json"] = True
    config["json_schema"] = {
        "type": "array",
        "items": {
            "type": "object",
            "properties": {
                "defect": {
                    "type": "string",
                    "enum": ["short", "open", "missing", "normal"]
                },
                "bbox": {
                    "type": "array",
                    "items": {"type": "integer"},
                    "minItems": 4,
                    "maxItems": 4
                },
                "repair": {"type": "string"},
                "confidence": {"type": "number", "minimum": 0, "maximum": 1}
            },
            "required": ["defect", "bbox", "repair"]
        }
    }
    
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    # ç®€å•éªŒè¯åˆå¹¶åæ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼Œç¡®ä¿æƒé‡æœªæŸå
    try:
        print("éªŒè¯åˆå¹¶åçš„æ¨¡å‹...")
        merged_model.eval()
        device = next(merged_model.parameters()).device
        dummy_image = Image.new("RGB", (224, 224), color="white")
        messages = [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": "Test"}]}]
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = processor(images=dummy_image, text=text, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = merged_model(**inputs)
        if torch.isnan(outputs.logits).any():
            raise RuntimeError("åˆå¹¶åæ¨¡å‹è¾“å‡ºNaNï¼Œæƒé‡å·²æŸåï¼")
        if outputs.logits.var() < 1e-6:
            raise RuntimeError("åˆå¹¶åæ¨¡å‹è¾“å‡ºæ–¹å·®è¿‡å°ï¼Œæƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼")
        print("âœ… åˆå¹¶æ¨¡å‹éªŒè¯é€šè¿‡")
    except Exception as e:
        print(f"âŒ åˆå¹¶åéªŒè¯å¤±è´¥: {e}")
        raise
    
    print("âœ… æ¨¡å‹åˆå¹¶å®Œæˆï¼")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   JSONæ ¼å¼çº¦æŸå·²å›ºåŒ–åˆ°config.json")
    
    # è®¡ç®—æ¨¡å‹å¤§å°
    total_size = 0
    for root, dirs, files in os.walk(output_dir):
        for file in files:
            file_path = os.path.join(root, file)
            if os.path.isfile(file_path):
                total_size += os.path.getsize(file_path)
    
    print(f"   æ¨¡å‹å¤§å°: {total_size / 1e9:.2f} GB")
    
    return merged_model


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹")
    parser.add_argument("--base_model", type=str, default="Qwen/Qwen3-VL-32B-Instruct", 
                       help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora_checkpoint", type=str, required=True,
                       help="LoRAæ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="./models/qwen3-vl-pcb",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--use_modelscope", action="store_true",
                       help="ä½¿ç”¨ModelScopeåŠ è½½æ¨¡å‹ï¼ˆè§£å†³ç½‘ç»œé—®é¢˜ï¼‰")
    
    args = parser.parse_args()
    
    merge_lora_weights(
        base_model_name=args.base_model,
        lora_checkpoint=args.lora_checkpoint,
        output_dir=args.output_dir,
        use_modelscope=args.use_modelscope
    )

