"""
Day 1-2: ç”µè·¯æ¿ç¼ºé™·MLLMå¾®è°ƒè„šæœ¬
ä½¿ç”¨Qwen3-VL-32B-Instruct + LoRAè¿›è¡Œå¾®è°ƒ
æ”¯æŒ HuggingFace å’Œ ModelScope ä¸¤ç§åŠ è½½æ–¹å¼
"""
import torch
import os
import numpy as np
from transformers import (
    Qwen3VLForConditionalGeneration,
    AutoProcessor,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from src.data.data_loader import load_pcb_dataset
import json

# å°è¯•å¯¼å…¥ ModelScope
try:
    from modelscope import snapshot_download
    MODELSCOPE_AVAILABLE = True
except ImportError:
    MODELSCOPE_AVAILABLE = False
    print("âš ï¸  ModelScopeæœªå®‰è£…ï¼Œå°†ä½¿ç”¨HuggingFaceã€‚å¦‚éœ€ä½¿ç”¨ModelScopeï¼Œè¯·è¿è¡Œ: pip install modelscope")


class PCBDataCollator:
    """ä½¿ç”¨å®˜æ–¹ chat template æ„é€ è¾“å…¥ä¸æ ‡ç­¾"""
    
    def __init__(self, processor, max_length=2048):
        self.processor = processor
        self.max_length = max_length
    
    def __call__(self, batch):
        images = [item["image"] for item in batch]

        messages = []
        for item in batch:
            user_msg = {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": item["question"] + "\nè¯·è¾“å‡ºJSONæ•°ç»„ã€‚"},
                ],
            }
            assistant_msg = {"role": "assistant", "content": item["answer"]}
            messages.append([user_msg, assistant_msg])

        texts = [
            self.processor.apply_chat_template(
                m, tokenize=False, add_generation_prompt=False
            )
            for m in messages
        ]

        inputs = self.processor(
            images=images,
            text=texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.max_length,
        )
        
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼Œå¹¶ä¸”ä¿æŒæ¢¯åº¦è¿æ¥
        # å¯¹äºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå›¾åƒä¸éœ€è¦æ¢¯åº¦ï¼ˆè§†è§‰ç¼–ç å™¨è¢«å†»ç»“ï¼‰ï¼Œ
        # ä½†input_idséœ€è¦é€šè¿‡æ¨¡å‹äº§ç”Ÿæ¢¯åº¦
        labels = inputs["input_ids"].clone().detach()  # labelsä¸éœ€è¦æ¢¯åº¦
        # input_idsä¿æŒåŸæ ·ï¼Œè®©æ¨¡å‹çš„å‰å‘ä¼ æ’­äº§ç”Ÿæ¢¯åº¦
        
        # è·å– assistant token IDï¼ˆQwen3-VL ä½¿ç”¨ç‰¹æ®Š tokenï¼‰
        assistant_token_id = None
        try:
            # ç›´æ¥å°è¯•è·å– assistant token ID
            if hasattr(self.processor.tokenizer, "convert_tokens_to_ids"):
                token_id = self.processor.tokenizer.convert_tokens_to_ids("<|assistant|>")
                if token_id is not None and token_id != self.processor.tokenizer.unk_token_id:
                    assistant_token_id = token_id
        except Exception:
            pass

        # æ©ç æ ‡ç­¾ï¼šåªä¿ç•™ assistant å“åº”éƒ¨åˆ†
        input_ids = inputs["input_ids"]  # ç¡®ä¿æ˜¯å¼ é‡
        if not isinstance(input_ids, torch.Tensor):
            input_ids = torch.tensor(input_ids)
        
        if assistant_token_id is not None and isinstance(assistant_token_id, (int, torch.Tensor)):
            # ç¡®ä¿ assistant_token_id æ˜¯æ ‡é‡
            if isinstance(assistant_token_id, torch.Tensor):
                assistant_token_id = assistant_token_id.item() if assistant_token_id.numel() == 1 else int(assistant_token_id)
            
            # æ‰¾åˆ°æ‰€æœ‰ assistant token çš„ä½ç½®
            for i in range(input_ids.shape[0]):
                # ç¡®ä¿æ¯”è¾ƒæ“ä½œè¿”å›å¼ é‡ï¼ˆä½¿ç”¨ torch.eq æ›´å®‰å…¨ï¼‰
                matches = torch.eq(input_ids[i], assistant_token_id)
                assistant_positions = torch.nonzero(matches, as_tuple=False)
                if len(assistant_positions) > 0:
                    start_pos = assistant_positions[0].item() + 1  # assistant token ä¹‹åå¼€å§‹
                    labels[i, :start_pos] = -100
        else:
            # å¦‚æœæ‰¾ä¸åˆ° assistant tokenï¼Œé€šè¿‡æ–‡æœ¬åˆ†ææ¥ç¡®å®šä½ç½®
            for i, text in enumerate(texts):
                if "<|assistant|>" in text:
                    # å¯¹å®Œæ•´æ–‡æœ¬è¿›è¡Œ tokenize
                    full_encoded = self.processor.tokenizer.encode(text, add_special_tokens=False)
                    # æ‰¾åˆ° "<|assistant|>" åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
                    assistant_part = text.split("<|assistant|>")[-1]
                    assistant_encoded = self.processor.tokenizer.encode(assistant_part, add_special_tokens=False)
                    # åœ¨å®Œæ•´åºåˆ—ä¸­æŸ¥æ‰¾ assistant éƒ¨åˆ†çš„èµ·å§‹ä½ç½®
                    if len(assistant_encoded) > 0:
                        for j in range(len(full_encoded) - len(assistant_encoded) + 1):
                            if full_encoded[j:j+len(assistant_encoded)] == assistant_encoded:
                                # æ‰¾åˆ°åŒ¹é…ä½ç½®ï¼Œæ©ç ä¹‹å‰çš„éƒ¨åˆ†
                                labels[i, :j] = -100
                                break
        
        # æ©ç  padding token
        pad_id = self.processor.tokenizer.pad_token_id
        if pad_id is not None:
            labels[labels == pad_id] = -100

        inputs["labels"] = labels
        return inputs


def setup_model(model_name: str = "Qwen/Qwen3-VL-32B-Instruct", 
                use_4bit: bool = True,
                device_map: str = "auto",
                model_revision: str = "main",
                use_modelscope: bool = False):
    """
    åŠ è½½å¹¶é…ç½®æ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼ˆHuggingFaceæ ¼å¼æˆ–ModelScopeæ ¼å¼ï¼‰
        use_4bit: æ˜¯å¦ä½¿ç”¨4-bité‡åŒ–åŠ è½½
        device_map: è®¾å¤‡æ˜ å°„ç­–ç•¥
        model_revision: æ¨¡å‹ç‰ˆæœ¬ï¼ˆcommit/tagï¼‰ï¼Œç”¨äºé”å®šç‰ˆæœ¬
    """
    print(f"åŠ è½½æ¨¡å‹: {model_name}")
    
    # ModelScope æ¨¡å‹æ˜ å°„
    modelscope_model_map = {
        "Qwen/Qwen3-VL-32B-Instruct": "qwen/Qwen3-VL-32B-Instruct",
        "qwen/Qwen3-VL-32B-Instruct": "qwen/Qwen3-VL-32B-Instruct",
    }
    
    # å¦‚æœä½¿ç”¨ModelScopeï¼Œå…ˆä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
    if use_modelscope and MODELSCOPE_AVAILABLE:
        if model_name in modelscope_model_map:
            modelscope_name = modelscope_model_map[model_name]
        elif model_name.startswith("Qwen/") or model_name.startswith("qwen/"):
            modelscope_name = model_name.replace("Qwen/", "qwen/")
        else:
            modelscope_name = model_name
        
        print(f"ä½¿ç”¨ModelScopeä¸‹è½½æ¨¡å‹: {modelscope_name}")
        try:
            # ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜
            local_model_path = snapshot_download(
                modelscope_name,
                cache_dir=os.getenv("MODELSCOPE_CACHE", "./modelscope_cache")
            )
            print(f"æ¨¡å‹å·²ä¸‹è½½åˆ°: {local_model_path}")
            model_name = local_model_path  # ä½¿ç”¨æœ¬åœ°è·¯å¾„
        except Exception as e:
            print(f"âš ï¸  ModelScopeä¸‹è½½å¤±è´¥: {e}")
            print("   å°†å°è¯•ä½¿ç”¨HuggingFaceåŠ è½½...")
            use_modelscope = False
    
    model_kwargs = {
        "trust_remote_code": True,
        "device_map": device_map,
        "torch_dtype": torch.float16,
        # ä¸å¼ºåˆ¶ä½¿ç”¨ flash_attention_2ï¼Œé¿å…ç¯å¢ƒæœªå®‰è£… FlashAttention2 æŠ¥é”™
        "local_files_only": False,  # å…è®¸ä»æœ¬åœ°åŠ è½½
        "revision": model_revision,
    }
    
    # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
    if os.path.exists(model_name) or model_name.startswith("./") or model_name.startswith("/"):
        model_kwargs["local_files_only"] = True
        print(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„: {model_name}")
    
    if use_4bit:
        from transformers import BitsAndBytesConfig
        # æ³¨æ„ï¼š4-bité‡åŒ–ä¸»è¦ç”¨äºæ¨ç†ï¼ŒLoRAè®­ç»ƒæ—¶å¯èƒ½ä¸å…¼å®¹
        # å¦‚æœé‡åˆ°æ¢¯åº¦é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨ --no_4bit ç¦ç”¨é‡åŒ–
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
        model_kwargs["quantization_config"] = quantization_config
        print("âš ï¸  ä½¿ç”¨4-bité‡åŒ–åŠ è½½æ¨¡å‹ã€‚")
        print("   å¦‚æœè®­ç»ƒæ—¶å‡ºç°æ¢¯åº¦é”™è¯¯ï¼Œè¯·ä½¿ç”¨ --no_4bit ç¦ç”¨é‡åŒ–ã€‚")
    
    # ä¸´æ—¶ç¦ç”¨ HuggingFace çš„åœ¨çº¿æ£€æŸ¥ï¼ˆå¦‚æœä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼‰
    if model_kwargs.get("local_files_only", False):
        os.environ["HF_HUB_OFFLINE"] = "1"
    
    try:
        model = Qwen3VLForConditionalGeneration.from_pretrained(model_name, **model_kwargs)
    finally:
        # æ¢å¤ç¯å¢ƒå˜é‡
        if "HF_HUB_OFFLINE" in os.environ:
            del os.environ["HF_HUB_OFFLINE"]
    
    # æ³¨æ„ï¼šè§†è§‰å¡”çš„å†»ç»“åº”è¯¥åœ¨LoRAåº”ç”¨ä¹‹åè¿›è¡Œ
    # å› ä¸ºLoRAä¼šåˆ›å»ºæ–°çš„å¯è®­ç»ƒå‚æ•°ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿è§†è§‰å¡”éƒ¨åˆ†è¢«å†»ç»“
    # ä½†è¿™é‡Œå…ˆä¸å†»ç»“ï¼Œç­‰LoRAåº”ç”¨åå†å¤„ç†
    
    return model


def setup_lora(model, r=16, alpha=32, dropout=0.05):
    """
    é…ç½®LoRA
    
    Args:
        model: åŸºç¡€æ¨¡å‹
        r: LoRA rank
        alpha: LoRA alpha
        dropout: LoRA dropout
    """
    # ç¦ç”¨ AWQ æ£€æµ‹ï¼ˆé¿å…å¯¼å…¥é”™è¯¯ï¼‰
    import os
    os.environ["PEFT_DISABLE_AWQ"] = "1"
    
    # æ‰¾åˆ°æ‰€æœ‰çº¿æ€§å±‚åç§°
    target_modules = []
    for name, module in model.named_modules():
        if any(layer in name for layer in ["q_proj", "k_proj", "v_proj", "o_proj",
                                            "gate_proj", "up_proj", "down_proj"]):
            target_modules.append(name.split('.')[-1])  # åªå–æœ€åä¸€å±‚åç§°
    
    # å»é‡
    target_modules = list(set(target_modules))
    
    print(f"LoRAç›®æ ‡æ¨¡å—: {target_modules}")
    
    lora_config = LoraConfig(
        r=r,  # ç¼ºé™·æ¨¡å¼æ¯”é€šç”¨è§†è§‰ç®€å•ï¼Œ16è¶³å¤Ÿ
        lora_alpha=alpha,
        target_modules=target_modules,
        lora_dropout=dropout,
        bias="none",
        task_type="CAUSAL_LM",  # Vision2Seqä½¿ç”¨å› æœè¯­è¨€æ¨¡å‹ä»»åŠ¡ç±»å‹
        modules_to_save=None,  # ä¸ä¿å­˜é¢å¤–æ¨¡å—
    )
    
    # ä¸´æ—¶ç¦ç”¨ AWQ ç›¸å…³çš„å¯¼å…¥
    import sys
    awq_modules = [k for k in sys.modules.keys() if 'awq' in k.lower()]
    for mod in awq_modules:
        if mod in sys.modules:
            del sys.modules[mod]
    
    try:
        # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
        model.train()
    model = get_peft_model(model, lora_config)
        # å†æ¬¡ç¡®ä¿è®­ç»ƒæ¨¡å¼
        model.train()
    except ImportError as e:
        if 'awq' in str(e).lower() or 'PytorchGELUTanh' in str(e):
            print("\n" + "="*60)
            print("âŒ é”™è¯¯ï¼šæ£€æµ‹åˆ° AWQ åº“å…¼å®¹æ€§é—®é¢˜")
            print("   è§£å†³æ–¹æ¡ˆï¼šå¸è½½è¿‡æ—¶çš„ awq åº“")
            print("   å‘½ä»¤ï¼špip uninstall -y autoawq awq")
            print("="*60 + "\n")
            raise RuntimeError(
                "AWQ åº“ä¸å½“å‰ transformers ç‰ˆæœ¬ä¸å…¼å®¹ã€‚\n"
                "è¯·è¿è¡Œ: pip uninstall -y autoawq awq\n"
                "ç„¶åé‡æ–°è¿è¡Œè®­ç»ƒå‘½ä»¤ã€‚"
            ) from e
        raise
    
    # æ‰“å°LoRAé€‚é…å™¨ä¿¡æ¯
    print("\nğŸ“Š LoRAé€‚é…å™¨ä¿¡æ¯:")
    if hasattr(model, "peft_config"):
        for adapter_name, adapter_config in model.peft_config.items():
            print(f"  é€‚é…å™¨: {adapter_name}")
            print(f"  - rank: {adapter_config.r}")
            print(f"  - alpha: {adapter_config.lora_alpha}")
            print(f"  - target_modules: {adapter_config.target_modules}")
    
    # ç¡®ä¿LoRAé€‚é…å™¨æ˜¯å¯è®­ç»ƒçš„
    # å¯¹äºé‡åŒ–æ¨¡å‹ï¼Œéœ€è¦æ˜¾å¼å¯ç”¨LoRAå‚æ•°çš„æ¢¯åº¦
    lora_params = []
    for name, param in model.named_parameters():
        if "lora" in name.lower():
            param.requires_grad = True
            lora_params.append(name)
    
    print(f"\nğŸ”§ æ‰¾åˆ° {len(lora_params)} ä¸ªLoRAå‚æ•°ç»„")
    if len(lora_params) > 0:
        print(f"  ç¤ºä¾‹LoRAå‚æ•°: {lora_params[:3]}...")
    
    # å†»ç»“è§†è§‰å¡”ï¼Œé¿å…åœ¨LoRAæ—¶ç ´åè§†è§‰ç‰¹å¾
    # æ³¨æ„ï¼šè¿™åº”è¯¥åœ¨LoRAåº”ç”¨ä¹‹åè¿›è¡Œï¼Œç¡®ä¿LoRAå‚æ•°ä¸å—å½±å“
    vision_frozen = 0
    for name, param in model.named_parameters():
        if ("vision" in name.lower() or "visual" in name.lower()) and "lora" not in name.lower():
            param.requires_grad = False
            vision_frozen += 1
    
    if vision_frozen > 0:
        print(f"ğŸ”’ å†»ç»“äº† {vision_frozen} ä¸ªè§†è§‰å¡”å‚æ•°")
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    print("\nğŸ“ˆ å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡:")
    model.print_trainable_parameters()
    
    # éªŒè¯è‡³å°‘æœ‰ä¸€äº›å‚æ•°æ˜¯å¯è®­ç»ƒçš„
    trainable_params = [p for p in model.parameters() if p.requires_grad]
    if len(trainable_params) == 0:
        raise RuntimeError("âŒ é”™è¯¯ï¼šæ²¡æœ‰å¯è®­ç»ƒçš„å‚æ•°ï¼LoRAé€‚é…å™¨å¯èƒ½æœªæ­£ç¡®åˆ›å»ºã€‚")
    
    # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°æ˜¯å¦æœ‰æ¢¯åº¦å‡½æ•°
    trainable_with_grad_fn = [p for p in trainable_params if p.requires_grad and p.grad_fn is not None]
    print(f"âœ… æ‰¾åˆ° {len(trainable_params)} ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œ{len(trainable_with_grad_fn)} ä¸ªæœ‰æ¢¯åº¦å‡½æ•°")
    
    if len(trainable_with_grad_fn) == 0 and len(trainable_params) > 0:
        print("âš ï¸  è­¦å‘Šï¼šå¯è®­ç»ƒå‚æ•°å­˜åœ¨ä½†æ²¡æœ‰æ¢¯åº¦å‡½æ•°ã€‚è¿™å¯èƒ½åœ¨è®­ç»ƒæ—¶å¯¼è‡´é—®é¢˜ã€‚")
    
    return model


def train_pcb_model(
    data_dir: str,
    output_dir: str = "./checkpoints/pcb_checkpoints",
    model_name: str = "Qwen/Qwen3-VL-32B-Instruct",
    max_steps: int = 2000,
    batch_size: int = 2,
    gradient_accumulation_steps: int = 8,
    learning_rate: float = 1e-4,
    use_4bit: bool = True,
    save_steps: int = 500,
    lora_r: int = 16,
    lora_alpha: int = 32,
    use_modelscope: bool = False,
    model_revision: str = "main",
):
    """
    è®­ç»ƒPCBç¼ºé™·æ£€æµ‹æ¨¡å‹
    
    Args:
        data_dir: æ•°æ®é›†ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        model_name: åŸºç¡€æ¨¡å‹åç§°
        max_steps: æœ€å¤§è®­ç»ƒæ­¥æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        learning_rate: å­¦ä¹ ç‡
        use_4bit: æ˜¯å¦ä½¿ç”¨4-bité‡åŒ–
        save_steps: ä¿å­˜æ­¥æ•°é—´éš”
        lora_r: LoRA rank
        lora_alpha: LoRA alpha
        model_revision: æ¨¡å‹ç‰ˆæœ¬ï¼ˆcommit/tagï¼‰ï¼Œç”¨äºé”å®šç‰ˆæœ¬
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®é›†
    print("åŠ è½½æ•°æ®é›†...")
    train_dataset = load_pcb_dataset(data_dir, augment=True)
    print(f"æ•°æ®é›†å¤§å°: {len(train_dataset)}")
    
    # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
    print("åŠ è½½æ¨¡å‹...")
    actual_model_path = model_name
    skip_download = False
    
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„ä¸”å·²å­˜åœ¨
    if os.path.exists(model_name) and os.path.exists(os.path.join(model_name, "config.json")):
        print(f"âœ… ä½¿ç”¨æœ¬åœ°å·²æœ‰æ¨¡å‹: {model_name}")
        actual_model_path = model_name
        skip_download = True
    # å¦‚æœä¸æ˜¯æœ¬åœ°è·¯å¾„ï¼Œä¸”éœ€è¦ä½¿ç”¨ModelScope
    elif use_modelscope and MODELSCOPE_AVAILABLE:
        modelscope_model_map = {
            "Qwen/Qwen3-VL-32B-Instruct": "qwen/Qwen3-VL-32B-Instruct",
            "qwen/Qwen3-VL-32B-Instruct": "qwen/Qwen3-VL-32B-Instruct",
        }
        if model_name in modelscope_model_map:
            modelscope_name = modelscope_model_map[model_name]
        elif model_name.startswith("Qwen/") or model_name.startswith("qwen/"):
            modelscope_name = model_name.replace("Qwen/", "qwen/")
        else:
            modelscope_name = model_name
        
        # æ£€æŸ¥ModelScopeç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰æ¨¡å‹ï¼ˆæ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ç¼“å­˜ä½ç½®ï¼‰
        cache_dir = os.getenv("MODELSCOPE_CACHE", "./modelscope_cache")
        possible_cache_paths = [
            os.path.join(cache_dir, modelscope_name.replace("/", "--")),
            os.path.join(cache_dir, modelscope_name),
            os.path.join(os.path.expanduser("~"), ".cache", "modelscope", "hub", modelscope_name),
        ]
        
        cached_path = None
        for cache_path in possible_cache_paths:
            if os.path.exists(cache_path) and os.path.exists(os.path.join(cache_path, "config.json")):
                cached_path = cache_path
                break
        
        if cached_path:
            print(f"âœ… ä½¿ç”¨ModelScopeç¼“å­˜ä¸­çš„æ¨¡å‹: {cached_path}")
            actual_model_path = cached_path
            skip_download = True
        else:
            # åªæœ‰åœ¨ç¼“å­˜ä¸­æ‰¾ä¸åˆ°æ—¶æ‰ä¸‹è½½
            print(f"ğŸ”„ ä»ModelScopeä¸‹è½½æ¨¡å‹: {modelscope_name}")
        print("   è¿™å°†é¿å…ç½‘ç»œè¿æ¥é—®é¢˜...")
        try:
            actual_model_path = snapshot_download(
                modelscope_name,
                    cache_dir=cache_dir
            )
            print(f"âœ… æ¨¡å‹å·²ä¸‹è½½åˆ°æœ¬åœ°: {actual_model_path}")
        except Exception as e:
            print(f"âŒ ModelScopeä¸‹è½½å¤±è´¥: {e}")
            print("   å°†å°è¯•ä½¿ç”¨HuggingFaceåŠ è½½...")
            use_modelscope = False
    elif use_modelscope and not MODELSCOPE_AVAILABLE:
        print("âš ï¸  æœªå®‰è£… ModelScopeï¼Œè¯·è¿è¡Œ: pip install modelscope")
        print("   å°†å°è¯•ä½¿ç”¨ HuggingFace åŠ è½½ï¼ˆå¯èƒ½éœ€è¦ç½‘ç»œè¿æ¥ï¼‰")
    
    # ç¦ç”¨ HuggingFace çš„åœ¨çº¿æ£€æŸ¥ï¼ˆå¦‚æœä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼‰
    if os.path.exists(actual_model_path) or actual_model_path.startswith("./") or actual_model_path.startswith("/"):
        print(f"ğŸ“ æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œç¦ç”¨ HuggingFace åœ¨çº¿æ£€æŸ¥")
        os.environ["HF_HUB_OFFLINE"] = "1"
        os.environ["TRANSFORMERS_OFFLINE"] = "1"
    
    try:
        model = setup_model(
            actual_model_path,
            use_4bit=use_4bit,
            device_map="auto",
            model_revision=model_revision,
            use_modelscope=use_modelscope,
        )  # å·²ä¸‹è½½ï¼Œç›´æ¥ä½¿ç”¨æœ¬åœ°è·¯å¾„
        
        # Processor ä¹Ÿä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼Œå¹¶é”å®šç‰ˆæœ¬
        processor_kwargs = {"trust_remote_code": True, "revision": model_revision}
        if os.path.exists(actual_model_path) or actual_model_path.startswith("./") or actual_model_path.startswith("/"):
            processor_kwargs["local_files_only"] = True
        
        # å…³é”®æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥ï¼Œé˜²æ­¢é™é»˜å›é€€
        # æ£€æŸ¥ tokenizer.jsonï¼ˆå¿…éœ€ï¼‰
        if not os.path.exists(os.path.join(actual_model_path, "tokenizer.json")):
            print(f"âŒ æœ¬åœ°æ¨¡å‹ç¼ºå°‘å…³é”®æ–‡ä»¶: tokenizer.json")
            print("   è¯·é‡æ–°ä¸‹è½½æˆ–æŒ‡å®šå®Œæ•´çš„æ¨¡å‹ç›®å½•")
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´: tokenizer.json")
        
        # æ£€æŸ¥ processor é…ç½®æ–‡ä»¶ï¼ˆpreprocessor_config.json æˆ– processor_config.json ä¹‹ä¸€å³å¯ï¼‰
        has_preprocessor = os.path.exists(os.path.join(actual_model_path, "preprocessor_config.json"))
        has_processor = os.path.exists(os.path.join(actual_model_path, "processor_config.json"))
        if not (has_preprocessor or has_processor):
            print(f"âš ï¸  æœ¬åœ°æ¨¡å‹ç¼ºå°‘ processor é…ç½®æ–‡ä»¶ï¼ˆpreprocessor_config.json æˆ– processor_config.jsonï¼‰")
            print("   å°†å°è¯•ç»§ç»­åŠ è½½ï¼Œå¦‚æœå¤±è´¥è¯·é‡æ–°ä¸‹è½½æ¨¡å‹")
        else:
            config_type = "preprocessor_config.json" if has_preprocessor else "processor_config.json"
            print(f"âœ… æ‰¾åˆ° processor é…ç½®æ–‡ä»¶: {config_type}")
        
        processor = AutoProcessor.from_pretrained(actual_model_path, **processor_kwargs)
    finally:
        # æ¢å¤ç¯å¢ƒå˜é‡
        if "HF_HUB_OFFLINE" in os.environ:
            del os.environ["HF_HUB_OFFLINE"]
        if "TRANSFORMERS_OFFLINE" in os.environ:
            del os.environ["TRANSFORMERS_OFFLINE"]
    
    # é…ç½®LoRA
    print("é…ç½®LoRA...")
    model = setup_lora(model, r=lora_r, alpha=lora_alpha)
    
    # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
    model.train()
    
    # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ¨¡å‹æœ‰å¯è®­ç»ƒå‚æ•°
    trainable_params = [p for p in model.parameters() if p.requires_grad]
    if len(trainable_params) == 0:
        raise RuntimeError(
            "âŒ è‡´å‘½é”™è¯¯ï¼šæ¨¡å‹æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼\n"
            "å¯èƒ½çš„åŸå› ï¼š\n"
            "1. LoRAé€‚é…å™¨æœªæ­£ç¡®åˆ›å»º\n"
            "2. 4-bité‡åŒ–ä¸LoRAä¸å…¼å®¹\n"
            "3. æ‰€æœ‰å‚æ•°è¢«æ„å¤–å†»ç»“\n"
            "å»ºè®®ï¼šå°è¯•ç¦ç”¨4-bité‡åŒ–ï¼ˆä½¿ç”¨ --no_4bit å‚æ•°ï¼‰"
        )
    
    # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°çš„æ¢¯åº¦çŠ¶æ€
    trainable_with_grad = sum(1 for p in trainable_params if p.requires_grad)
    print(f"\nâœ… è®­ç»ƒå‰éªŒè¯ï¼š{trainable_with_grad}/{len(trainable_params)} ä¸ªå¯è®­ç»ƒå‚æ•°å·²å¯ç”¨æ¢¯åº¦")
    
    # ç¡®ä¿æ¨¡å‹çš„æ‰€æœ‰å¯è®­ç»ƒå‚æ•°éƒ½æ­£ç¡®è®¾ç½®
    for name, param in model.named_parameters():
        if param.requires_grad:
            # ç¡®ä¿å‚æ•°ä¸æ˜¯detachedçš„
            if not param.is_leaf or param.grad_fn is not None:
                # å¦‚æœå‚æ•°æœ‰grad_fnï¼Œè¯´æ˜å®ƒä¾èµ–äºå…¶ä»–è®¡ç®—ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                pass
            # å¯¹äºLoRAå‚æ•°ï¼Œå®ƒä»¬åº”è¯¥æ˜¯leafèŠ‚ç‚¹ï¼Œä½†requires_grad=True
            if "lora" in name.lower() and not param.requires_grad:
                print(f"âš ï¸  è­¦å‘Šï¼šLoRAå‚æ•° {name} çš„ requires_grad=Falseï¼Œå¼ºåˆ¶è®¾ç½®ä¸ºTrue")
                param.requires_grad = True
    
    # å¦‚æœä½¿ç”¨4-bité‡åŒ–ï¼Œç»™å‡ºè­¦å‘Š
    if use_4bit:
        print("\n" + "="*60)
        print("âš ï¸  é‡è¦æç¤ºï¼šä½¿ç”¨4-bité‡åŒ–è¿›è¡ŒLoRAè®­ç»ƒ")
        print("   å¦‚æœè®­ç»ƒæ—¶å‡ºç° 'does not require grad' é”™è¯¯ï¼Œ")
        print("   è¯·ä½¿ç”¨ --no_4bit å‚æ•°ç¦ç”¨4-bité‡åŒ–ã€‚")
        print("="*60 + "\n")
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = PCBDataCollator(processor, max_length=2048)
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=learning_rate,
        max_steps=max_steps,
        save_steps=save_steps,
        fp16=True,
        save_only_model=False,  # ä¿å­˜å®Œæ•´æ¨¡å‹ï¼Œä¾¿äºç›´æ¥æ¨ç†
        logging_steps=10,  # æ›´é¢‘ç¹çš„æ—¥å¿—ï¼Œä¾¿äºæ—©æœŸå‘ç°é—®é¢˜
        warmup_ratio=0.1,
        weight_decay=0.01,
        report_to="none",
        remove_unused_columns=False,  # ä¿ç•™æ‰€æœ‰åˆ—ï¼Œç”±data_collatorå¤„ç†
        max_grad_norm=0.3,  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
        dataloader_pin_memory=True,
        gradient_checkpointing=False,  # æš‚æ—¶ç¦ç”¨ï¼Œé¿å…æ¢¯åº¦è¿æ¥é—®é¢˜
        fp16_full_eval=False,
        logging_nan_inf_filter=True,
    )
    
    # æ—©åœå›è°ƒï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)] if len(train_dataset) < 1000 else []
    
    # 8bit AdamW ä¼˜åŒ–å™¨ï¼ˆä¸4bitåŠ è½½é…åˆæ›´ç¨³ï¼‰
    try:
        from bitsandbytes.optim import AdamW8bit
        optimizer = AdamW8bit(model.parameters(), lr=learning_rate)
        optimizers = (optimizer, None)
    except Exception as e:
        print(f"âš ï¸  æœªå®‰è£…bitsandbytesï¼Œå›é€€åˆ°é»˜è®¤AdamW: {e}")
        optimizers = (None, None)
    
    # è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
        callbacks=callbacks,
        optimizers=optimizers,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # åˆå¹¶LoRAæƒé‡å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹
    print("æ­£åœ¨åˆå¹¶LoRAæƒé‡å¹¶ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    model.eval()
    if hasattr(model, "merge_and_unload"):
        merged_model = model.merge_and_unload()
        print("âœ… LoRAæƒé‡å·²åˆå¹¶")
    else:
        merged_model = model
        print("âš ï¸ æœªæ£€æµ‹åˆ° merge_and_unloadï¼Œç›´æ¥ä¿å­˜å½“å‰æ¨¡å‹")

    final_model_path = os.path.join(output_dir, "final")
    merged_model.save_pretrained(final_model_path)
    processor.save_pretrained(final_model_path)
    
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼å®Œæ•´æ¨¡å‹ä¿å­˜åœ¨: {final_model_path}")
    
    return merged_model, processor


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="è®­ç»ƒPCBç¼ºé™·æ£€æµ‹æ¨¡å‹")
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®é›†ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="./checkpoints/pcb_checkpoints", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen3-VL-32B-Instruct", help="åŸºç¡€æ¨¡å‹åç§°")
    parser.add_argument("--max_steps", type=int, default=2000, help="æœ€å¤§è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--batch_size", type=int, default=2, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--save_steps", type=int, default=500, help="ä¿å­˜checkpointçš„æ­¥æ•°é—´éš”")
    parser.add_argument("--lora_r", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    parser.add_argument("--no_4bit", action="store_true", help="ä¸ä½¿ç”¨4-bité‡åŒ–")
    parser.add_argument("--use_modelscope", action="store_true", help="ä½¿ç”¨ModelScopeåŠ è½½æ¨¡å‹ï¼ˆè§£å†³ç½‘ç»œé—®é¢˜ï¼‰")
    parser.add_argument("--model_revision", type=str, default="main", help="æ¨¡å‹ç‰ˆæœ¬ï¼ˆcommit hashæˆ–tagï¼‰")
    
    args = parser.parse_args()
    
    train_pcb_model(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        model_name=args.model_name,
        max_steps=args.max_steps,
        batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        use_4bit=not args.no_4bit,
        save_steps=args.save_steps,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        use_modelscope=args.use_modelscope,
        model_revision=args.model_revision,
    )

